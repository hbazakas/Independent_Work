{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe Cleaner\n",
    "\n",
    "This notebook walks through the process of taking in scraped recipes and outputting recipes with names familiar to the FooDB database. To do so, we have used pretrained BERT embeddings. These allow us to map recipe ingredient strings to their appropriate match in FooDB's database.\n",
    "\n",
    "This process is lengthy. If you do not have a need to rerun it yourself, the output files are available. They are:\n",
    "* __all_recipes_cleaned.pkl__\n",
    "* __all_recipes_cleaned.csv__\n",
    "* __all_recipes_cleaned_indices.pkl__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrybazakas/opt/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "import time\n",
    "import pickle\n",
    "import csv\n",
    "import zipfile\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('content_dict_weight.pkl', 'rb') as f:\n",
    "    content_dict_weight = pickle.load(f)\n",
    "with open('content_dict_presence.pkl', 'rb') as f:\n",
    "    content_dict_presence = pickle.load(f)\n",
    "with open('content_dict_complete.pkl', 'rb') as f:\n",
    "    content_dict_complete = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recipes_simplified = pd.read_csv(\"All_Recipes_Simplified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9209 9461 9461\n"
     ]
    }
   ],
   "source": [
    "weight_keys = content_dict_weight.keys()\n",
    "presence_keys = content_dict_presence.keys()\n",
    "complete_keys = content_dict_complete.keys() #presence and complete have the same keys. weight has fewer keys\n",
    "\n",
    "print(len(weight_keys), len(presence_keys), len(complete_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_names = []\n",
    "for i in complete_keys:\n",
    "    if type(i) == str:\n",
    "        ingredient_names.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight_embeddings = sbert_model.encode(weight_keys)\n",
    "complete_embeddings = sbert_model.encode(ingredient_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping names to FooDB Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match(ingredient):\n",
    "    ''' \n",
    "    Function to intake any ingredient (as from Food.com) and return a name that is acceptable within Content.csv\n",
    "    '''\n",
    "    best_match = 0.65 #can start this at something >0 if you want to implement a stricter cutoff.\n",
    "    best_match_name = \"Bad Match\" #could change this too in order to set name to something w zero compounds\n",
    "    ingredient_embedding = sbert_model.encode(ingredient)\n",
    "    for i in range(len(complete_embeddings)):\n",
    "        match = float(cosine_similarity(ingredient_embedding.reshape(1,-1), \n",
    "                                        complete_embeddings[i].reshape(1,-1)))\n",
    "        if match > best_match:\n",
    "            best_match = match\n",
    "            best_match_name = ingredient_names[i]\n",
    "    #if best_match > cutoff:\n",
    "    return best_match_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_ingredients = {}\n",
    "def content_map(ingredient):\n",
    "    if ingredient in ingredient_names:\n",
    "        matched_ingredients[ingredient] = ingredient\n",
    "        return ingredient\n",
    "    elif ingredient in matched_ingredients.keys():\n",
    "        return matched_ingredients[ingredient]\n",
    "    else:\n",
    "        match = best_match(ingredient)\n",
    "        matched_ingredients[ingredient] = match\n",
    "        return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.828763723373413 Apple\n",
      "2.1678812503814697 Apple\n"
     ]
    }
   ],
   "source": [
    "def matcher(ingredient):\n",
    "    s = time.time()\n",
    "    a = best_match(ingredient)\n",
    "    print(time.time() - s, a)\n",
    "    s = time.time()\n",
    "    b = content_map(ingredient)\n",
    "    print(time.time() - s, b)\n",
    "    \n",
    "matcher(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 recipes processed 0.01658177375793457\n",
      "10 recipes processed 4.260057687759399\n",
      "100 recipes processed 570.0142889022827\n",
      "500 recipes processed 1685.5871918201447\n",
      "1000 recipes processed 2583.545888900757\n",
      "5000 recipes processed 5839.20241189003\n",
      "10000 recipes processed 7819.295591831207\n",
      "20000 recipes processed 10090.306432008743\n",
      "50000 recipes processed 13766.288568735123\n",
      "100000 recipes processed 17049.469326734543\n",
      "170000 recipes processed 19847.147416830063\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "cleaned_recipes_full = []\n",
    "cleaned_recipes_full_indices = []\n",
    "#for i in range(10):\n",
    "for i in range(all_recipes_simplified.shape[0]):\n",
    "    recipe = []\n",
    "    bad_matches = 0\n",
    "    try:\n",
    "        for j in range(len(ast.literal_eval(all_recipes_simplified._c0[i]))):\n",
    "            matched_name = content_map(ast.literal_eval(all_recipes_simplified._c0[i])[j][0])\n",
    "            if matched_name == \"Bad Match\":\n",
    "                bad_matches += 1\n",
    "            recipe.append([matched_name, ast.literal_eval(all_recipes_simplified._c0[i])[j][1]])\n",
    "        if bad_matches == 0:\n",
    "            cleaned_recipes_full.append(recipe)\n",
    "            cleaned_recipes_full_indices.append(i)\n",
    "    except:\n",
    "        pass\n",
    "    if i in [1, 10, 100, 500, 1000, 5000, 10000, 20000, 50000, 100000, 170000]:\n",
    "        progress = time.time() - start\n",
    "        print(i, \"recipes processed\", progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_recipes_cleaned.pkl', 'wb') as f:\n",
    "    pickle.dump(cleaned_recipes_full, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_recipes_cleaned.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(cleaned_recipes_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_recipes_cleaned_indices.pkl', 'wb') as f:\n",
    "    pickle.dump(cleaned_recipes_full_indices, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "* In all, all but 1,699 of the recipes were matched to FooDB friendly names. This leaves us with a cleaned dataset, __all_recipes_cleaned__, containing 176,286 recipes.\n",
    "\n",
    "* __all_recipes_cleaned_indices__, saved above as a pkl, is a list of indices from __All_Recipes_Simplified.csv__ that were not removed due to poor ingredient matches. This can be used if you want to recocile the cleaned recipes with the originals.\n",
    "\n",
    "* __recipe_match__, a function included below, is an example of how one could compare the recipes before and after cleaning using indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_recipes_full_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1699 176286\n"
     ]
    }
   ],
   "source": [
    "print(len(all_recipes_simplified) -  len(cleaned_recipes_full), len(cleaned_recipes_full))\n",
    "\n",
    "#1,699 recipes were removed due to bad ingredient matches. If you want to pair up the cleaned recipes with\n",
    "#the originals, use the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipe_match(index):\n",
    "    '''Take in index from all_recipes_cleaned, output original recipe as seen in all_recipes_simplified.'''\n",
    "    return ast.literal_eval(all_recipes_simplified.iloc[cleaned_recipes_full_indices[index]]._c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['chicken legs', 1360.78],\n",
       " ['onion', 150.0],\n",
       " ['soy sauce', 232.0],\n",
       " ['brown sugar', 220.0],\n",
       " ['ground ginger', 4.0],\n",
       " ['minced garlic cloves', 6.0],\n",
       " ['dry sherry', 60.0]]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_match(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Chicken spread', 1360.78],\n",
       " ['onion', 150.0],\n",
       " ['Soy sauce', 232.0],\n",
       " ['Sugar, brown', 220.0],\n",
       " ['Spices, ginger, ground', 4.0],\n",
       " ['Spices, garlic powder', 6.0],\n",
       " ['Sherry, dry', 60.0]]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_recipes_full[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
